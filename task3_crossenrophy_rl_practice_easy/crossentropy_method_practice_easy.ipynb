{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning problems with crossentropy method. We'll follow-up by scaling everything up and using neural network policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Downloading pygame-2.6.1-cp313-cp313-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/10.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.6 MB 1.0 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.5/10.6 MB 1.0 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.0/10.6 MB 1.1 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.3/10.6 MB 1.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.6/10.6 MB 1.3 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.1/10.6 MB 1.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.4/10.6 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 2.9/10.6 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.4/10.6 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 3.9/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.9/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.5/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.5/10.6 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.7/10.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.7/10.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.7/10.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.0/10.6 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.0/10.6 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.0/10.6 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.2/10.6 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.2/10.6 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.2/10.6 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.2/10.6 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.2/10.6 MB 1.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.5/10.6 MB 1.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.8/10.6 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 5.8/10.6 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 5.8/10.6 MB 1.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.0/10.6 MB 941.9 kB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.0/10.6 MB 941.9 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 898.4 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 898.4 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 6.6/10.6 MB 892.3 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.8/10.6 MB 907.3 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.8/10.6 MB 907.3 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.8/10.6 MB 907.3 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.8/10.6 MB 907.3 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.8/10.6 MB 907.3 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 7.1/10.6 MB 817.6 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 7.3/10.6 MB 838.1 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.3/10.6 MB 838.1 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.3/10.6 MB 838.1 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 807.1 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 807.1 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 807.1 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 807.1 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.9/10.6 MB 753.4 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.9/10.6 MB 753.4 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.9/10.6 MB 753.4 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 8.1/10.6 MB 732.3 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 8.1/10.6 MB 732.3 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 8.4/10.6 MB 728.2 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 8.4/10.6 MB 728.2 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 8.7/10.6 MB 722.5 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.7/10.6 MB 722.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 9.2/10.6 MB 741.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 748.2 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 748.2 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 748.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.7/10.6 MB 740.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.7/10.6 MB 740.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.7/10.6 MB 740.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.7/10.6 MB 740.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 10.0/10.6 MB 711.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.6 MB 718.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.6 MB 721.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 716.9 kB/s  0:00:14\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Something went wrong with pygame. This should never happen.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m env = gym.make(\u001b[33m\"\u001b[39m\u001b[33mTaxi-v3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m env.reset()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\core.py:329\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\n\u001b[32m    326\u001b[39m     \u001b[38;5;28mself\u001b[39m, *args, **kwargs\n\u001b[32m    327\u001b[39m ) -> Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[32m    328\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:51\u001b[39m, in \u001b[36mOrderEnforcing.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:53\u001b[39m, in \u001b[36mPassiveEnvChecker.render\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_render = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.render(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:316\u001b[39m, in \u001b[36menv_render_passive_checker\u001b[39m\u001b[34m(env, *args, **kwargs)\u001b[39m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    311\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m env.render_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env.render_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[32m    312\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    313\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv.render_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    314\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m result = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# TODO: Check that the result is correct\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:290\u001b[39m, in \u001b[36mTaxiEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._render_text()\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\envs\\toy_text\\taxi.py:309\u001b[39m, in \u001b[36mTaxiEnv._render_gui\u001b[39m\u001b[34m(self, mode)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    306\u001b[39m         \u001b[38;5;28mself\u001b[39m.window = pygame.Surface(WINDOW_SIZE)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28mself\u001b[39m.window \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    310\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mSomething went wrong with pygame. This should never happen.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.clock = pygame.time.Clock()\n",
      "\u001b[31mAssertionError\u001b[39m: Something went wrong with pygame. This should never happen."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize policy (0.5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = #code here\n",
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game (0.5pts)\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(policy,t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = np.random.choice(range(n_actions), p = policy[s])\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #Record state, action and add up reward to states,actions and total_reward accordingly. \n",
    "        #add state\n",
    "        #add action\n",
    "        #add reward\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r = generate_session(policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float,np.float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see the initial reward distribution\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(policy,t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards,bins=20);\n",
    "plt.vlines([np.percentile(sample_rewards,50)],[0],[100],label=\"50'th percentile\",color='green')\n",
    "plt.vlines([np.percentile(sample_rewards,90)],[0],[100],label=\"90'th percentile\",color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy method steps (1pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "    \n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\" \n",
    "    reward_threshold = #recalc threshold. hint : np.percentile\n",
    "    \n",
    "    elite_states = []\n",
    "    \n",
    "    if(len(np.array(states_batch[0]).shape) == 1):\n",
    "        elite_states = np.hstack(#states)\n",
    "    else:\n",
    "        elite_states = np.vstack(#states)\n",
    "    elite_actions = np.hstack(#actions)\n",
    "    \n",
    "    return elite_states,elite_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_batch = np.array([\n",
    "    np.array([1,2,3]),   #game1\n",
    "    np.array([4,2,0,2]), #game2\n",
    "    np.array([3,1])      #game3\n",
    "])\n",
    "\n",
    "actions_batch = np.array([\n",
    "    np.array([0,2,4]),   #game1\n",
    "    np.array([3,2,0,1]), #game2\n",
    "    np.array([3,3])      #game3\n",
    "])\n",
    "rewards_batch = np.array([\n",
    "    3,         #game1\n",
    "    4,         #game2\n",
    "    5,         #game3\n",
    "])\n",
    "\n",
    "test_result_0 = select_elites(states_batch,actions_batch,rewards_batch,percentile=0)\n",
    "test_result_40 = select_elites(states_batch,actions_batch,rewards_batch,percentile=30)\n",
    "test_result_90 = select_elites(states_batch,actions_batch,rewards_batch,percentile=90)\n",
    "test_result_100 = select_elites(states_batch,actions_batch,rewards_batch,percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "   and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "        np.all(test_result_40[1] ==[3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3,1]) and \\\n",
    "        np.all(test_result_90[1] == [3,3]),\\\n",
    "        \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3,1]) and\\\n",
    "       np.all(test_result_100[1] == [3,3]),\\\n",
    "        \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(elite_states,elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "    \n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "    \n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "    \n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_policy = np.zeros([n_states,n_actions])\n",
    "    \n",
    "    #<Your code here: update probabilities for actions given elite states & actions>\n",
    "    #Don't forget to set 1/n_actions for all actions in unvisited states.\n",
    "    \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elite_states, elite_actions = ([1, 2, 3, 4, 2, 0, 2, 3, 1], [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
    "\n",
    "\n",
    "new_policy = update_policy(elite_states,elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(new_policy>=0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(axis=-1),1), \"Your new policy should be a valid probability distribution over actions\"\n",
    "reference_answer = np.array([\n",
    "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.5       ,  0.        ,  0.        ,  0.5       ,  0.        ],\n",
    "       [ 0.        ,  0.33333333,  0.66666667,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.5       ,  0.5       ]])\n",
    "print(new_policy[:4,:5])\n",
    "assert np.allclose(new_policy[:4,:5],reference_answer)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop (1pts)\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_progress(rewards_batch,log, reward_range=[-990,+10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch,percentile)\n",
    "    log.append([mean_reward,threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward,threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0],label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1],label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(rewards_batch,range=reward_range);\n",
    "    plt.vlines([np.percentile(rewards_batch,percentile)],[0],[100],label=\"percentile\",color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset policy just in case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sessions = 250  #sample this many sessions\n",
    "percentile = 70  #take this percent of session with highest rewards\n",
    "learning_rate = 0.5  #add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    %time sessions = [generate_session(policy) for _ in range(n_sessions)]\n",
    "    \n",
    "    states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
    "\n",
    "    elite_states, elite_actions = select_elites(np.array(states_batch),np.array(actions_batch),np.array(rewards_batch),percentile=50)\n",
    "    \n",
    "    new_policy = update_policy(elite_states,elite_actions)\n",
    "    \n",
    "    policy = #update policy\n",
    "    \n",
    "    #display results on chart\n",
    "    show_progress(rewards_batch,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular crossentropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -100 to a near-optimal score and then descends back into -50/-100. This is in part because the environment has some innate randomness. Namely, the starting points of passenger/driver change from episode to episode.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples. Show all precentiles on one graph and all n_samples on another\n",
    "- __1.2__ (1 pts) Tune the algorithm to end up with positive average score.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "policy = 1/6. * np.ones([n_states, n_actions])\n",
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)\n",
    "s,a,r = generate_session(policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float,np.float]\n",
    "policy = np.ones([n_states,n_actions])/n_actions \n",
    "stepCounter = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sessions = 250  #sample this many sessions\n",
    "percentiles = []  #take this percent of session with highest rewards\n",
    "learning_rate = 0.5  #add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "mean = []\n",
    "for percentile in percentiles:\n",
    "    curMean = []\n",
    "    policy = np.ones([n_states,n_actions])/n_actions\n",
    "    for i in range(stepCounter):\n",
    "\n",
    "        %time sessions = [generate_session(policy) for _ in range(n_sessions)]\n",
    "\n",
    "        states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
    "\n",
    "        elite_states, elite_actions = select_elites(np.array(states_batch),np.array(actions_batch),np.array(rewards_batch),percentile=50)\n",
    "\n",
    "        #set new policy\n",
    "        #update policy\n",
    "        #add mean value\n",
    "        clear_output(True)\n",
    "    mean.append(curMean)\n",
    "    #display results on chart\n",
    "    #show_progress(rewards_batch,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do different percentiles affect training efficiency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sessions_ar = []  #sample this many sessions\n",
    "perc = 70  #take this percent of session with highest rewards\n",
    "learning_rate = 0.5  #add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "mean = []\n",
    "for n_sessions in n_sessions_ar:\n",
    "    curMean = []\n",
    "    policy = np.ones([n_states,n_actions])/n_actions\n",
    "    for i in range(stepCounter):\n",
    "\n",
    "        %time sessions = [generate_session(policy) for _ in range(n_sessions)]\n",
    "\n",
    "        states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
    "\n",
    "        elite_states, elite_actions = select_elites(np.array(states_batch),\n",
    "                                                    np.array(actions_batch),np.array(rewards_batch),percentile=perc)\n",
    "\n",
    "        #set new policy\n",
    "        #update policy\n",
    "        #add mean value\n",
    "        clear_output(True)\n",
    "    mean.append(curMean)\n",
    "    #display results on chart\n",
    "    #show_progress(rewards_batch,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stabilize positive rewards by averaging policy across 10 games (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(policy,t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    #code here\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sessions = 75  #sample this many sessions\n",
    "percentile = 70  #take this percent of session with highest rewards\n",
    "learning_rate = 0.2  #add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "mean = []\n",
    "policy = np.ones([n_states,n_actions])/n_actions\n",
    "for i in range(1000):\n",
    "\n",
    "    %time sessions = [generate_session(policy) for _ in range(n_sessions)]\n",
    "\n",
    "    states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
    "\n",
    "    elite_states, elite_actions = select_elites(np.array(states_batch),np.array(actions_batch),np.array(rewards_batch),percentile=50)\n",
    "\n",
    "    #code here\n",
    "\n",
    "    show_progress(rewards_batch,log)\n",
    "    \n",
    "    if(np.mean(rewards_batch) > 7):\n",
    "        print(\"win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digging deeper: approximate crossentropy with neural nets (2 pts)\n",
    "\n",
    "In this section we will train a neural network policy for continuous state space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "env = gym.make(\"CartPole-v0\").env  #if you see \"<classname> has no attribute .env\", remove .env or update gym\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case use NN as a black box. All your should know that it is more complex than a tabular method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities hint: predict_proba\n",
    "        \n",
    "        \n",
    "        a = np.random.choice(np.arange(n_actions), p=probs)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        \n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "    \n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\" \n",
    "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
    "    \n",
    "    elite_states = []\n",
    "    \n",
    "    if(len(np.array(states_batch[0]).shape) == 1):\n",
    "        elite_states = np.hstack(states_batch[rewards_batch >= reward_threshold])\n",
    "    else:\n",
    "        elite_states = np.vstack(states_batch[rewards_batch >= reward_threshold])\n",
    "    elite_actions = np.hstack(actions_batch[rewards_batch >= reward_threshold])\n",
    "    \n",
    "    return elite_states,elite_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this classification model use fit(states, actions). In this case we are training classificator to predict the correct class (action) in a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sessions = 100\n",
    "percentile = 70\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_sessions)]\n",
    "\n",
    "    states_batch,actions_batch,rewards_batch = map(np.array,zip(*sessions))\n",
    "    print(states_batch.shape)\n",
    "\n",
    "    elite_states, elite_actions = select_elites(states_batch,actions_batch,rewards_batch,percentile=50)\n",
    "    \n",
    "    #fit agent\n",
    "\n",
    "    show_progress(rewards_batch,log,reward_range=[0,np.max(rewards_batch)])\n",
    "    \n",
    "    if np.mean(rewards_batch)> 190:\n",
    "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (1 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe what you did here.  Preferably with plot/report to support it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
